# Local RAG Chat API (Session-Aware, Streaming)

A **local Retrieval-Augmented Generation (RAG) system** built with **Mistral-7B, FAISS, and FastAPI**, supporting **session-aware document uploads, intent-aware retrieval, and real-time streaming responses** â€” without relying on external LLM APIs.

This project demonstrates **production-grade RAG design patterns** such as document isolation, active-file tracking, adaptive retrieval strategies, and streaming inference.

---

## ğŸš€ Features

- Local LLM inference using **Mistral-7B (Hugging Face Transformers)**
- **Session-aware document uploads** with active-file tracking
- **Intent-aware retrieval**
  - Full-document retrieval for summaries and document-level questions
  - Semantic top-k retrieval for fact-based queries
- **FAISS vector search** with SentenceTransformers (E5)
- Robust **document chunking**
  - Supports `.docx` and `.ipynb`
- **Real-time token streaming** using Server-Sent Events (SSE)
- Conversation memory per session
- CPU / GPU compatible with quantized models
- Optional **Ollama fallback** for local inference
- No external APIs required

---

## ğŸ§  Architecture

```
Client (UI)
    â”‚
    â–¼
FastAPI
â”œâ”€â”€ Upload Endpoint
â”‚   â””â”€â”€ Chunking + Embeddings
â”‚       â””â”€â”€ FAISS Index
â”‚
â”œâ”€â”€ Chat Endpoint
â”‚   â”œâ”€â”€ Session Memory
â”‚   â”œâ”€â”€ Active Document Resolver
â”‚   â”œâ”€â”€ Intent-Aware Retrieval
â”‚   â””â”€â”€ Prompt Construction
â”‚
â””â”€â”€ Streaming Endpoint (SSE)
    â””â”€â”€ Token-by-token output
```

---

## ğŸ§± Tech Stack

- **LLM**: Mistral-7B (Transformers)
- **Embeddings**: `intfloat/e5-small-v2`
- **Vector Store**: FAISS
- **Backend**: FastAPI, Uvicorn
- **Streaming**: Server-Sent Events (SSE)
- **Chunking**: Custom logic for DOCX / IPYNB
- **Optional**: Ollama for fallback inference

---

## ğŸ“‚ Project Structure

```
hf-rag-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # API routes + streaming
â”‚   â”œâ”€â”€ rag.py               # RAG logic (retrieval + generation)
â”‚   â”œâ”€â”€ chunking.py          # File loading and chunking
â”‚   â”œâ”€â”€ memory.py            # Session memory & active-file tracking
â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â””â”€â”€ ollama_client.py     # Optional Ollama fallback
â”‚
â”œâ”€â”€ index/                   # FAISS index (generated at runtime)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ›  Setup

### 1ï¸âƒ£ Create virtual environment

```bash
python -m venv .venv
source .venv/bin/activate    # macOS / Linux
.venv\Scripts\activate       # Windows
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ (Optional) Set Hugging Face cache location

```bash
# Windows
setx HF_HOME E:\hf_cache

# macOS / Linux
export HF_HOME=/path/to/hf_cache
```

### 4ï¸âƒ£ Run the server

```bash
uvicorn app.main:app --reload
```

Server runs at:

```
http://127.0.0.1:8000
```

---

## ğŸ“¤ Upload a Document (Session-Aware)

```http
POST /upload
```

**Parameters**

| Name         | Description                 |
| ------------ | --------------------------- |
| `session_id` | Unique session identifier   |
| `file`       | `.docx` or `.ipynb` file    |

**Example response**

```json
{
  "file": "resume.docx",
  "type": "docx",
  "chunks_added": 6,
  "active_file": "resume.docx"
}
```

Uploading a new document automatically updates the **active document** for that session.

---

## ğŸ’¬ Chat with the Document

```http
GET /chat
```

**Query Parameters**

| Name         | Description               |
| ------------ | ------------------------- |
| `session_id` | Unique session identifier |
| `query`      | Your question             |

### Example queries

- "Summarize the document"
- "What skills are mentioned?"
- "Explain the projects section"

---

## âš¡ Streaming Chat (SSE)

```http
GET /chat/stream
```

Streams tokens in real time:

```
data: [START]
data: This document is a professional resume...
data: It highlights experience in...
data: [END]
```

---

## ğŸ§  Retrieval Strategy

| Query Type                       | Retrieval Behavior          |
| -------------------------------- | --------------------------- |
| "Summary of the document"        | All chunks from active file |
| "What kind of document is this?" | All chunks from active file |
| "What tools are mentioned?"      | Semantic top-k              |
| "Where is AWS used?"             | Semantic + file filter      |

This design prevents:

- Cross-document leakage
- Empty summaries
- Hallucinated context

---

## ğŸ”’ Session Isolation

- Each session maintains:
  - Independent conversation memory
  - An active document
- Queries are **strictly scoped** to the active document
- Uploading a new file updates the session context automatically

---

## ğŸ§ª Known Limitations

- Large models may require CPU offloading or GPU memory
- FAISS index is local (single-node)
- Authentication not included (demo-focused)

---

## ğŸŒ± Future Improvements

- Multi-document selection per session
- Hybrid retrieval (BM25 + FAISS)
- Chunk-level citations in UI
- RAG evaluation metrics
- Docker + GPU deployment
- Full web UI with document explorer


## ğŸ“œ License

MIT License