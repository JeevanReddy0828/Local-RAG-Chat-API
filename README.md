# ðŸ” Local RAG Chat API

[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/downloads/)
[![PyTorch 2.4](https://img.shields.io/badge/PyTorch-2.4-EE4C2C.svg?logo=pytorch&logoColor=white)](https://pytorch.org/)
[![CUDA 12.1](https://img.shields.io/badge/CUDA-12.1-76B900.svg?logo=nvidia&logoColor=white)](https://developer.nvidia.com/cuda-toolkit)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688.svg?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![FAISS](https://img.shields.io/badge/FAISS-Vector_Search-blue.svg)](https://github.com/facebookresearch/faiss)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg?logo=docker&logoColor=white)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A **production-grade, local Retrieval-Augmented Generation (RAG) system** built with **Mistral-7B, FAISS, and FastAPI**. Features session-aware document management, intent-aware retrieval strategies, real-time streaming responses, and a built-in evaluation framework â€” all running locally without external API dependencies.

---

## ðŸ“‹ Table of Contents

- [Key Features](#-key-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [API Reference](#-api-reference)
- [Retrieval Strategy](#-retrieval-strategy)
- [Evaluation Framework](#-evaluation-framework)
- [Docker Deployment](#-docker-deployment)
- [Configuration](#-configuration)
- [Known Limitations](#-known-limitations)
- [Future Roadmap](#-future-roadmap)
- [License](#-license)

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| **ðŸ¤– Local LLM Inference** | Mistral-7B via Ollama â€” no external APIs, full data privacy |
| **ðŸ“ Session-Aware Uploads** | Per-session document isolation with active-file tracking |
| **ðŸŽ¯ Intent-Aware Retrieval** | Adaptive strategy: full-document for summaries, semantic top-k for facts |
| **âš¡ Real-Time Streaming** | Server-Sent Events (SSE) for token-by-token output |
| **ðŸ§  Conversation Memory** | Multi-turn context retention per session |
| **ðŸ“Š Evaluation Framework** | Built-in Recall@K and answer similarity metrics |
| **ðŸ³ Docker + GPU Ready** | Production deployment with NVIDIA CUDA support |
| **ðŸŒ Web UI Included** | Clean, functional chat interface out of the box |

---

## ðŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              CLIENT LAYER                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚    â”‚   Web UI     â”‚         â”‚  REST Client â”‚         â”‚   cURL/SDK   â”‚      â”‚
â”‚    â”‚  (index.html)â”‚         â”‚  (Postman)   â”‚         â”‚              â”‚      â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚           â”‚                        â”‚                        â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                        â”‚                        â”‚
            â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              API LAYER (FastAPI)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚  POST /upload  â”‚    â”‚  POST /chat    â”‚    â”‚ GET /chat/streamâ”‚          â”‚
â”‚    â”‚                â”‚    â”‚                â”‚    â”‚     (SSE)      â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚            â”‚                     â”‚                     â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                     â”‚                     â”‚
             â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              RAG ENGINE                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        DOCUMENT PROCESSING                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚  â”‚  chunking.py â”‚  â”‚   ingest.py  â”‚  â”‚  E5 Embedder â”‚              â”‚   â”‚
â”‚  â”‚  â”‚  .docx/.ipynbâ”‚  â”‚  .txt/.md    â”‚  â”‚  (384-dim)   â”‚              â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         RETRIEVAL LAYER                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚                    Intent Detection                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  "summarize" / "overview" â†’ Full Document Retrieval          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Fact-based queries       â†’ Semantic Top-K (FAISS)           â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                              â”‚                                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚  â”‚  â”‚ FAISS Index  â”‚  â”‚ Active File    â”‚  â”‚ Session      â”‚           â”‚   â”‚
â”‚  â”‚  â”‚ (IndexFlatIP)â”‚  â”‚ Filter         â”‚  â”‚ Isolation    â”‚           â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        GENERATION LAYER                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚  â”‚  memory.py   â”‚  â”‚ Prompt Build â”‚  â”‚ Ollama Clientâ”‚              â”‚   â”‚
â”‚  â”‚  â”‚  (History)   â”‚  â”‚ (Context+Q)  â”‚  â”‚ (Mistral-7B) â”‚              â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              STORAGE LAYER                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚    data/                                                                    â”‚
â”‚    â”œâ”€â”€ raw/                    # Uploaded documents                         â”‚
â”‚    â”‚   â””â”€â”€ {filename}                                                       â”‚
â”‚    â””â”€â”€ index/                  # Per-session FAISS indexes                  â”‚
â”‚        â””â”€â”€ {session_id}/                                                    â”‚
â”‚            â”œâ”€â”€ faiss.index     # Vector embeddings                          â”‚
â”‚            â””â”€â”€ meta.json       # Chunk metadata + sources                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ§± Tech Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **LLM** | Mistral-7B (via Ollama) | Local text generation |
| **Deep Learning** | PyTorch 2.4 + CUDA 12.1 | GPU-accelerated inference |
| **Embeddings** | `intfloat/e5-small-v2` | 384-dim semantic vectors |
| **Vector Store** | FAISS (IndexFlatIP) | Similarity search |
| **Backend** | FastAPI + Uvicorn | Async API server |
| **Streaming** | Server-Sent Events | Real-time token output |
| **Document Parsing** | python-docx, json | DOCX/IPYNB support |
| **Evaluation** | RapidFuzz | Answer similarity scoring |
| **Containerization** | Docker + NVIDIA Container Toolkit | GPU-accelerated deployment |

---

## ðŸ“‚ Project Structure

```
hf-rag-api/
â”‚
â”œâ”€â”€ app/                          # Core application
â”‚   â”œâ”€â”€ __init__.py               # Package init
â”‚   â”œâ”€â”€ main.py                   # FastAPI routes & endpoints
â”‚   â”œâ”€â”€ rag.py                    # RAG engine (retrieval + generation)
â”‚   â”œâ”€â”€ chunking.py               # Document parsing (DOCX/IPYNB)
â”‚   â”œâ”€â”€ ingest.py                 # Text file ingestion
â”‚   â”œâ”€â”€ memory.py                 # Session memory & active-file tracking
â”‚   â”œâ”€â”€ config.py                 # Pydantic settings
â”‚   â”œâ”€â”€ ollama_client.py          # Ollama API client (sync + streaming)
â”‚   â””â”€â”€ eval.py                   # Evaluation framework
â”‚
â”œâ”€â”€ ui/                           # Simple HTML frontend
â”‚   â””â”€â”€ index.html                # Vanilla JS chat interface
â”‚
â”œâ”€â”€ ui-react/                     # React frontend (recommended)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx               # Main React component
â”‚   â”‚   â””â”€â”€ main.jsx              # Entry point
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data/                         # Runtime data (gitignored)
â”‚   â”œâ”€â”€ raw/                      # Uploaded documents
â”‚   â””â”€â”€ index/                    # Per-session FAISS indexes
â”‚
â”œâ”€â”€ eval_data.jsonl               # Evaluation test cases
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ Dockerfile                    # Container build
â”œâ”€â”€ docker-compose.yml            # GPU orchestration
â”œâ”€â”€ .gitignore                    # Git exclusions
â”œâ”€â”€ .env.example                  # Environment template
â””â”€â”€ README.md                     # Documentation
```

---

## ðŸš€ Quick Start

### Prerequisites

- **Python 3.9 - 3.12** (Python 3.13+ not yet supported by PyTorch)
- [Ollama](https://ollama.ai/) with Mistral model
- **For GPU acceleration:**
  - NVIDIA GPU with CUDA support
  - CUDA 12.1+ and cuDNN 9+
  - PyTorch 2.4+ with CUDA support
  - [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) (for Docker)

### 1ï¸âƒ£ Clone & Setup Environment

```bash
git clone https://github.com/yourusername/hf-rag-api.git
cd hf-rag-api
```

**Linux/macOS:**
```bash
python3.11 -m venv .venv
source .venv/bin/activate
```

**Windows (PowerShell):**
```powershell
# Check available Python versions
py --list

# Create venv with Python 3.11 (or 3.12)
py -3.11 -m venv .venv311
.venv311\Scripts\activate
```

> âš ï¸ **Important:** PyTorch requires Python 3.9-3.12. If you have Python 3.13+, you must specify an older version when creating the virtual environment.

### 2ï¸âƒ£ Install Dependencies

**For GPU (CUDA 12.1) - Recommended:**
```bash
# Install PyTorch with CUDA support first
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install remaining dependencies
pip install -r requirements.txt

# Verify CUDA is available
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
```

**For CPU only:**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install -r requirements.txt
```

### 3ï¸âƒ£ Start Ollama with Mistral

**Linux/macOS:**
```bash
# Install Ollama (if not installed)
curl -fsSL https://ollama.ai/install.sh | sh

# Pull and run Mistral
ollama pull mistral
ollama serve  # Runs on port 11434 by default
```

**Windows:**
```powershell
# Download and install from https://ollama.ai/download/windows
# Then:
ollama pull mistral
ollama serve
```

> ðŸ“ **Note:** Ollama may run on port `11434` or `11435`. Check your Ollama output and update `.env` if needed:
> ```
> OLLAMA_BASE_URL=http://localhost:11435
> ```

### 4ï¸âƒ£ Configure (Optional)

```bash
cp .env.example .env
# Edit .env to customize ports, models, etc.
```

### 5ï¸âƒ£ Run the Server

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 6ï¸âƒ£ Open the UI

**Option A: Simple HTML UI**

Navigate to **http://localhost:8000** â†’ auto-redirects to the basic chat UI.

**Option B: React UI (Recommended)**

```bash
cd ui-react
npm install
npm run dev
```

Opens at **http://localhost:3000** with:
- ðŸŒ™ Dark theme minimal design
- ðŸ“± Fully responsive
- âš¡ Real-time streaming
- ðŸ“Š Index statistics
- ðŸ’¾ Session management

---

## ðŸ“¡ API Reference

### Upload Document

```http
POST /upload
Content-Type: multipart/form-data
```

| Parameter | Type | Description |
|-----------|------|-------------|
| `session_id` | string | Unique session identifier |
| `file` | file | Document (.docx, .ipynb, .txt, .md) |

**Response:**
```json
{
  "session_id": "user123",
  "file": "resume.docx",
  "type": "docx",
  "chunks_added": 8,
  "active_file": "resume.docx"
}
```

### Chat (Synchronous)

```http
POST /chat
Content-Type: application/json
```

**Request:**
```json
{
  "session_id": "user123",
  "query": "What skills are mentioned in the document?"
}
```

**Response:**
```json
{
  "answer": "The document mentions Python, FastAPI, and machine learning skills...",
  "sources": [
    {"source": "resume.docx", "best_score": 0.847}
  ],
  "active_file": "resume.docx"
}
```

### Chat (Streaming)

```http
GET /chat/stream?session_id=user123&query=Summarize%20the%20document
Accept: text/event-stream
```

**Response (SSE):**
```
data: [START]
data: The document is a professional resume...
data: It highlights experience in...
data: [END]
```

### Clear Session Index

```http
POST /index/clear?session_id=user123
```

### Clear Session Memory

```http
POST /memory/clear?session_id=user123
```

---

## ðŸŽ¯ Retrieval Strategy

The system uses **intent-aware retrieval** to optimize context selection:

| Query Pattern | Detection | Retrieval Strategy |
|---------------|-----------|-------------------|
| "Summarize the document" | Keyword match | **All chunks** from active file |
| "What is this document about?" | Keyword match | **All chunks** from active file |
| "Overview of the document" | Keyword match | **All chunks** from active file |
| "What skills are mentioned?" | Semantic | **Top-K** (default: 4) via FAISS |
| "Where is Python used?" | Semantic | **Top-K** with active-file filter |

### Intent Detection Triggers

```python
DOC_LEVEL_TRIGGERS = [
    "summary of the document",
    "summarize the document",
    "summarize this document",
    "what is this document",
    "describe this document",
    "what kind of document",
    "overview of the document",
    "summary",
]
```

### Why This Matters

| Problem | Solution |
|---------|----------|
| Empty summaries from top-k only | Full-document retrieval for summary queries |
| Cross-document contamination | Active-file filtering |
| Hallucinated context | Strict source scoping |

---

## ðŸ“Š Evaluation Framework

Comprehensive evaluation with **intrinsic** (retrieval quality) and **extrinsic** (answer quality) metrics.

### Metrics Overview

| Category | Metric | Description |
|----------|--------|-------------|
| **Intrinsic** | Precision@K | Fraction of retrieved docs that are relevant |
| | Recall@K | Fraction of relevant docs that are retrieved |
| | MRR | Mean Reciprocal Rank - position of first relevant result |
| | NDCG | Normalized Discounted Cumulative Gain - ranking quality |
| | Similarity Stats | Embedding similarity distribution |
| **Extrinsic** | Answer Relevance | How well answer addresses the question (0-100) |
| | Faithfulness | Is answer grounded in retrieved context (0-100) |
| | Answer Similarity | Fuzzy match with expected answer (0-100) |
| | Task Success Rate | Binary success/fail for specific task types |
| | Hallucination Score | Facts not in source documents (lower is better) |

### Prepare Test Data

Create `eval_data.jsonl`:
```jsonl
{"question": "What programming languages are mentioned?", "expected_answer": "Python, JavaScript, SQL", "task_type": "factual", "difficulty": "easy"}
{"question": "Summarize the document", "expected_answer": "A professional resume for a software engineer", "task_type": "summary", "difficulty": "medium"}
{"question": "What certifications does the person have?", "expected_answer": "AWS Machine Learning Associate", "task_type": "extraction", "difficulty": "easy"}
```

### Run Evaluation

```bash
# Basic evaluation
python -m app.evaluation --eval-file eval_data.jsonl

# Save results to JSON
python -m app.evaluation --eval-file eval_data.jsonl --output results.json
```

### Sample Output

```
================================================================================
RAG SYSTEM EVALUATION REPORT
================================================================================
Timestamp: 2024-12-20T15:30:00
Samples Evaluated: 10

----------------------------------------
INTRINSIC METRICS (Retrieval Quality)
----------------------------------------
  Precision@K:      0.8500
  Recall@K:         0.9000
  MRR:              0.9200
  NDCG:             0.8800
  Avg Similarity:   0.7650

----------------------------------------
EXTRINSIC METRICS (Answer Quality)
----------------------------------------
  Answer Relevance: 82.50/100
  Faithfulness:     78.30/100
  Answer Similarity:71.20/100
  Task Success Rate:80.00%
  Hallucination:    21.70/100 (lower is better)
  Avg Latency:      1250ms

----------------------------------------
METRICS BY TASK TYPE
----------------------------------------
  [SUMMARY] (n=3)
    Precision:    0.9000
    Success Rate: 100.00%

  [FACTUAL] (n=4)
    Precision:    0.8500
    Success Rate: 75.00%

  [EXTRACTION] (n=3)
    Precision:    0.8000
    Success Rate: 66.67%
================================================================================
```

---

## ðŸ³ Docker Deployment

### Prerequisites for GPU

```bash
# Verify NVIDIA driver
nvidia-smi

# Install NVIDIA Container Toolkit (if not installed)
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### GPU-Accelerated (Recommended)

```bash
# Ensure NVIDIA Container Toolkit is installed
docker compose up --build
```

### CPU-Only

```bash
# Modify docker-compose.yml to remove GPU reservation
docker compose up --build
```

### Docker Compose Configuration

```yaml
services:
  rag-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

---

## âš™ï¸ Configuration

All settings via environment variables or `.env` file:

| Variable | Default | Description |
|----------|---------|-------------|
| `DATA_DIR` | `data` | Base data directory |
| `RAW_DIR` | `data/raw` | Uploaded documents |
| `INDEX_DIR` | `data/index` | FAISS indexes |
| `EMBED_MODEL` | `intfloat/e5-small-v2` | Embedding model |
| `TOP_K` | `4` | Retrieval count |
| `CHUNK_MAX_CHARS` | `1400` | Max chunk size |
| `CHUNK_OVERLAP_CHARS` | `250` | Chunk overlap |
| `OLLAMA_ENABLED` | `true` | Use Ollama |
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama endpoint |
| `OLLAMA_MODEL` | `mistral` | LLM model |
| `MAX_NEW_TOKENS` | `256` | Generation limit |
| `TEMPERATURE` | `0.2` | Sampling temperature |

---

## âš ï¸ Known Limitations

| Limitation | Mitigation |
|------------|------------|
| Single-node FAISS | Suitable for demo/small-scale; use Pinecone/Weaviate for production |
| No authentication | Add OAuth2/JWT for production deployment |
| In-memory session state | Add Redis for horizontal scaling |
| Ollama dependency | Ensure Ollama is running before API starts |

---

## ðŸ—º Future Roadmap

- [ ] **Hybrid Retrieval** â€” BM25 + FAISS fusion
- [ ] **Multi-Document Selection** â€” Query across multiple active files
- [ ] **Chunk-Level Citations** â€” UI displays source chunks with highlights
- [ ] **Redis Session Store** â€” Horizontal scaling support
- [ ] **Authentication** â€” OAuth2 / API key support
- [ ] **Observability** â€” OpenTelemetry tracing + Prometheus metrics
- [ ] **PDF Support** â€” PyMuPDF integration
- [ ] **Reranking** â€” Cross-encoder reranking for improved precision

---

## ðŸ§‘â€ðŸ’» Author

**Jeevan Reddy**  
Software Engineer | ML/NLP Enthusiast

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://linkedin.com/in/yourprofile)
[![GitHub](https://img.shields.io/badge/GitHub-Follow-black)](https://github.com/yourusername)

---
## ðŸ“Œ Resume Bullet Point

> Designed and built a **production-grade local RAG system** using **PyTorch, CUDA, Mistral-7B, FAISS, and FastAPI** with **intent-aware retrieval** (full-document vs. semantic top-k), **session isolation**, **SSE streaming**, and a **built-in evaluation framework** â€” demonstrating GPU-accelerated ML inference and production patterns for document Q&A without external API dependencies.

---

## ðŸ“œ License

MIT License â€” see [LICENSE](LICENSE) for details.