{"cells":[{"cell_type":"markdown","metadata":{"id":"rJ2Nc0MLftSH"},"source":["# Instruction\n","Plase use <b>Python 3</b> in Jupyter Notebook.\n","\n","This lab focuses on classifcation and prediction. We will practice classification methods on a real world wetland mapping dataset. Each data sample contains several numeric features and a binary class label (0 for dry land, 1 for wetland). \n","\n","<b>Requirement</b>\n","- <font color=red>Plese upload your Jupyter Notebook with required Data files and Python script files all in the SAME zipped FOLDER</font>\n","\n","- Please MAKE SURE your codes run smoothly without bugs in Jupyter Notebook with Python 3. \n","    \n","- <font color=red>Codes with bugs or errors that cannot run through in Jupyter notebook will be graded as ZERO for that part.</font> \n","\n","## Statement of Contribution\n","\n","We both tried the homework on our own. Ramey typed up programming solutions, and Lay reviewed them. Ramey worked on extra credit problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dO7N-8-5ftSJ"},"outputs":[],"source":["# Load libraries\n","import pandas as pd\n","from sklearn.ensemble import BaggingClassifier # Bagging Classifier\n","from sklearn.ensemble import RandomForestClassifier # Random Forest Classifier\n","from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n","from sklearn.linear_model import LogisticRegression # Import Logistic Regression Classifier\n","from sklearn.svm import SVC # Import SVM classifier\n","from sklearn.model_selection import train_test_split # Import train_test_split function\n","from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"]},{"cell_type":"markdown","metadata":{"id":"hvfo5UBOftSK"},"source":["## Data loading\n","Please use the following codes to load the data. The training and test data are both saved in common separated values (CSV) format. The last column is class label. \n","\n","PLEASE RUN TEH CODES BELOW."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143,"status":"ok","timestamp":1676863758488,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"tu-KJMzzftSK","outputId":"23faf8c2-b3ff-4ece-8823-edd30aecf605"},"outputs":[{"name":"stdout","output_type":"stream","text":["   near infra red   red  green  blue  class\n","0              123  132    115   133      0\n","1              152  150    119   187      1\n","2              169  166    143   192      1\n","3               55   49     43    97      0\n","4              141  135    117   181      1\n","   near infra red   red  green  blue  class\n","0              137  140    129   150      0\n","1              169  162    140   193      1\n","2              124  110     89   162      1\n","3              105  104     99   153      1\n","4              105  102     88   173      1\n"]}],"source":["#note: class 0 for dry land, class 1 for wetland\n","col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n","features =  ['near infra red ', 'red', 'green', 'blue'] \n","\n","# load dataset\n","train_dat = pd.read_csv(\"train.csv\", header = None, names = col_names)\n","print(train_dat.head())\n","\n","test_dat = pd.read_csv(\"test.csv\", header = None, names = col_names)\n","print(test_dat.head())\n","\n","# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n","X_train = train_dat.drop('class', axis='columns')\n","Y_train = train_dat['class']\n","\n","# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n","X_test = test_dat.drop('class', axis='columns')\n","Y_test = test_dat['class']"]},{"cell_type":"markdown","metadata":{"id":"v7E0aTooftSK"},"source":["## Part 1: Evaluate the performance of decision tree"]},{"cell_type":"markdown","metadata":{"id":"dZ8K307eftSK"},"source":["Run the codes below to train a decision tree, and make predictions on test samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dTcH5_c_ftSL"},"outputs":[],"source":["# Create Decision Tree classifer object\n","dt = DecisionTreeClassifier()\n","\n","# Train Decision Tree Classifer\n","dt = dt.fit(X_train, Y_train)\n","\n","#Predict the response for test dataset\n","Y_pred = dt.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"hIG90uEwftSL"},"source":["Run the codes below to compute the \"Overall Accuracy\", as well as Precision, Recall, and F-score for the Wetland class (class 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1676863766787,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"A79zXyk1ftSL","outputId":"7008e2fe-12d4-46fe-9875-50c2dcc7de8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.783\n","0.7411504424778761\n","0.7701149425287356\n","0.7553551296505072\n"]}],"source":["# Fill in codes to calculate the values below; You can add any codes, but don't change the variable names\n","\n","accuracy = metrics.accuracy_score(Y_test, Y_pred)\n","\n","precision_wet = metrics.precision_score(Y_test, Y_pred)\n","\n","recall_wet = metrics.recall_score(Y_test, Y_pred)\n","\n","F_wet = metrics.f1_score(Y_test, Y_pred)\n","\n","print(accuracy)\n","print(precision_wet)\n","print(recall_wet)\n","print(F_wet)"]},{"cell_type":"markdown","metadata":{"id":"jBqjYagFftSL"},"source":["<font color=red>ANSWER THE QUESTION BELOW.</font> How do you think about the performance shown by different metrics above. Is accuracy a good metric to reflect classification performance? Why? You can discuss your answer as a string. And print it out. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1676863835400,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"Vs7v9Ol9ftSL","outputId":"751fc5e1-7011-4f7f-e17d-a5b172522862"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","The accuracy of the model (81.1%) isn't high enough, it should be above 85%. Therefore we can say the model isn't good enough. Accuracy is a correct metric to check a model's performance but it not always the one that gives best reflection of the model.\n","\n"]}],"source":["#MY ANSWER IS xxxxx\n","ans = '''\n","The accuracy of the model (81.1%) isn't high enough, it should be above 85%. Therefore we can say the model isn't good enough. Accuracy is a correct metric to check a model's performance but it not always the one that gives best reflection of the model.\n","'''\n","print(ans)"]},{"cell_type":"markdown","metadata":{"id":"vFfqytSCftSM"},"source":["## Part 2: Overfitting issues\n","<font color=red>PLEASE COMPLETE TEH CODES BELOW.</font> Please re-train the decision tree model with a smaller number of training samples. <font color=red>DO NOT change the tree model parameters (e.g., minimum leaf node size) from Part 1</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":860,"status":"ok","timestamp":1676863840969,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"mi5bRG_oftSM","outputId":"3d022e42-5e4e-4b6b-c6c3-d8f523d5ae30"},"outputs":[{"name":"stdout","output_type":"stream","text":["   near infra red   red  green  blue  class\n","0              143  150    145   156      0\n","1              151  143    120   183      1\n","2              100   98     86   156      0\n","3              140  137    119   182      0\n","4              147  147    138   160      0\n","   near infra red   red  green  blue  class\n","0              137  140    129   150      0\n","1              169  162    140   193      1\n","2              124  110     89   162      1\n","3              105  104     99   153      1\n","4              105  102     88   173      1\n","\n","Metrics of Model on Training Data\n","1.0\n","1.0\n","1.0\n","1.0\n","\n","Metrics of Model on Test Data\n","Accuracy: 0.701\n","Precision Score: 0.660377358490566\n","Recall Score: 0.6436781609195402\n","F1 Score: 0.6519208381839348\n"]}],"source":["# Re-run the training and testing based on a smaller training set (smalltrain.csv).\n","#note: class 0 for dry land, class 1 for wetland\n","col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n","features = ['near infra red ', 'red', 'green', 'blue'] \n","\n","# load dataset\n","train_dat = pd.read_csv(\"smalltrain.csv\", header = None, names = col_names)\n","print(train_dat.head())\n","\n","test_dat = pd.read_csv(\"test.csv\", header = None, names = col_names)\n","print(test_dat.head())\n","\n","# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n","X_train = train_dat.drop('class', axis = 'columns')\n","Y_train = train_dat['class']\n","\n","# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n","X_test = test_dat.drop('class', axis = 'columns')\n","Y_test = test_dat['class']\n","\n","# Train Decision Tree Classifer\n","dt = dt.fit(X_train, Y_train)\n","\n","# Re-evaluate the trained model on test data, print out accuracy, precision, recall, F-score\n","Y_pred = dt.predict(X_train)\n","accuracy = metrics.accuracy_score(Y_train, Y_pred)\n","precision_wet = metrics.precision_score(Y_train, Y_pred)\n","recall_wet = metrics.recall_score(Y_train, Y_pred)\n","F_wet = metrics.f1_score(Y_train, Y_pred)\n","\n","print('\\nMetrics of Model on Training Data')\n","print(accuracy)\n","print(precision_wet)\n","print(recall_wet)\n","print(F_wet)\n","\n","Y_pred = dt.predict(X_test)\n","accuracy = metrics.accuracy_score(Y_test, Y_pred)\n","precision_wet = metrics.precision_score(Y_test, Y_pred)\n","recall_wet = metrics.recall_score(Y_test, Y_pred)\n","F_wet = metrics.f1_score(Y_test, Y_pred)\n","\n","print('\\nMetrics of Model on Test Data')\n","print(f'Accuracy: {accuracy}')\n","print(f'Precision Score: {precision_wet}')\n","print(f'Recall Score: {recall_wet}')\n","print(f'F1 Score: {F_wet}')"]},{"cell_type":"markdown","metadata":{"id":"8m0M3sI3ftSM"},"source":["<font color=red>ANSWER THE QUESTION BELOW.</font> What do you observe when the number of training samples decrese? Is this overfitting or underfitting?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":129,"status":"ok","timestamp":1676864109752,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"_mrQl1tWftSM","outputId":"e4aadbdc-4110-48a3-fcc1-9bb89544c33d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","As the number of training sample reduced, accuracy fell down by approximately 10% which is not good. Not only that, the other values such as precision and recall have also dropped causing the performance of the model to decrease. Since, the values have reduced a lot but accuracy is anyway over 70%. Therefore this problem is of overfitting. \n","\n"]}],"source":["#MY Answer: xxxxxxxxxxx\n","ans = '''\n","As the number of training sample reduced, accuracy fell down by approximately 10% which is not good. Not only that, the other values such as precision and recall have also dropped causing the performance of the model to decrease. Since, the values have reduced a lot but accuracy is anyway over 70%. Therefore this problem is of overfitting. \n","'''\n","\n","print(ans)"]},{"cell_type":"markdown","metadata":{"id":"VaUPZjspftSM"},"source":["We now do another experiment to mitigate the effect of overfitting by decreasing the model complexity. We keep using the small training data (smalltrain.csv) that leads to overfitting. We decrease model complexity by having a larger minimum leaf node size (\"min_sample_split=30\"). "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":156,"status":"ok","timestamp":1676864114576,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"LLFqNnRiftSM","outputId":"2311e816-c92c-4f44-c33e-4f7a0ee59e2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["   near infra red   red  green  blue  class\n","0              143  150    145   156      0\n","1              151  143    120   183      1\n","2              100   98     86   156      0\n","3              140  137    119   182      0\n","4              147  147    138   160      0\n","   near infra red   red  green  blue  class\n","0              137  140    129   150      0\n","1              169  162    140   193      1\n","2              124  110     89   162      1\n","3              105  104     99   153      1\n","4              105  102     88   173      1\n","\n","Metrics of Model on Test Data\n","0.757\n","0.6889763779527559\n","0.8045977011494253\n","0.7423117709437965\n"]}],"source":["col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n","features =  ['near infra red ', 'red', 'green', 'blue'] \n","\n","# load dataset\n","train_dat = pd.read_csv(\"smalltrain.csv\", header=None, names=col_names)\n","print(train_dat.head())\n","\n","test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n","print(test_dat.head())\n","\n","# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n","\n","X_train = train_dat.drop('class', axis = 'columns')\n","Y_train = train_dat['class']\n","\n","# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n","\n","X_test = test_dat.drop('class', axis = 'columns')\n","Y_test = test_dat['class']\n","\n","# Create Decision Tree classifer object\n","dt_simple = DecisionTreeClassifier(min_samples_split=30) ### Make model much simplier by requiring 30 samples to split\n","\n","# Train Decision Tree Classifer\n","dt_simple = dt_simple.fit(X_train, Y_train)\n","\n","#Predict the response for test dataset\n","Y_pred = dt_simple.predict(X_test)\n","\n","accuracy = metrics.accuracy_score(Y_test, Y_pred)\n","precision_wet = metrics.precision_score(Y_test, Y_pred)\n","recall_wet = metrics.recall_score(Y_test, Y_pred)\n","F_wet = metrics.f1_score(Y_test, Y_pred)\n","\n","print('\\nMetrics of Model on Test Data')\n","print(accuracy)\n","print(precision_wet)\n","print(recall_wet)\n","print(F_wet)"]},{"cell_type":"markdown","metadata":{"id":"Yq_Wn_AGftSN"},"source":["<font color=red>ANSWER THE QUESTION BELOW.</font> Compare the results above with the results from the beginning of Part 2 (smalltrain.csv with the original decision tree without decreasing model complexity), what did you observe? "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1676864220415,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"Wsp_Ex2hftSN","outputId":"712ebbd2-22c4-49fb-c054-4934ff593d98"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","The output metric value increased so the performance is better compared to the previous.\n","\n"]}],"source":["#MY ANSWER IS xxxxx\n","ans = '''\n","The output metric value increased so the performance is better compared to the previous.\n","'''\n","print(ans)"]},{"cell_type":"markdown","metadata":{"id":"nsYdxIrBftSN"},"source":["## Part 3: Compare different model on the same test data\n","In this part, you will train other types of models and evaluate on the test data. You will compare their classification performance.\n","\n","<font color=red>PLEASE COMPLETE TEH CODES BELOW.</font> PLEASE USE THE SAME TRAINING AND TEST DATA as in Part 1."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":147,"status":"ok","timestamp":1676864223116,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"kMpWwcQUftSN","outputId":"49593c28-39cb-4a53-fb7a-23c58d7c4146"},"outputs":[{"name":"stdout","output_type":"stream","text":["   near infra red   red  green  blue  class\n","0              123  132    115   133      0\n","1              152  150    119   187      1\n","2              169  166    143   192      1\n","3               55   49     43    97      0\n","4              141  135    117   181      1\n","   near infra red   red  green  blue  class\n","0              137  140    129   150      0\n","1              169  162    140   193      1\n","2              124  110     89   162      1\n","3              105  104     99   153      1\n","4              105  102     88   173      1\n","0.757\n","0.7307692307692307\n","0.6988505747126437\n","0.7144535840188013\n"]}],"source":["col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n","features =  ['near infra red ', 'red', 'green', 'blue'] \n","\n","# load dataset\n","train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n","print(train_dat.head())\n","\n","test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n","print(test_dat.head())\n","\n","# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n","X_train = train_dat.drop('class', axis='columns')\n","Y_train = train_dat['class']\n","\n","# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n","X_test = test_dat.drop('class', axis='columns')\n","Y_test = test_dat['class']\n","\n","# train a logistic regression model; e.g., use the LogisticRegression model in Scikit-Learn.\n","# using parameter \"solver='liblinear'\"\n","lr = LogisticRegression(solver='liblinear')\n","lr = lr.fit(X_train, Y_train)\n","Y_pred = lr.predict(X_test)\n","\n","# evalute the logistic regression model on test data. \n","accuracy = metrics.accuracy_score(Y_test, Y_pred)\n","precision_wet = metrics.precision_score(Y_test, Y_pred)\n","recall_wet = metrics.recall_score(Y_test, Y_pred)\n","F_wet = metrics.f1_score(Y_test, Y_pred)\n","\n","print(accuracy)\n","print(precision_wet)\n","print(recall_wet)\n","print(F_wet)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136,"status":"ok","timestamp":1676864238257,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"Mk7Gw6REftSN","outputId":"5a6fddab-3915-4770-abf5-92688ef39184"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.81\n","0.7406679764243614\n","0.8666666666666667\n","0.798728813559322\n"]}],"source":["# train a Support Vector Machine (SVM) model, e.g., the SVC model in Scikit-Learn, choose parameters approprioately\n","\n","# Please find the SVC function in Scikit-Learn, use parameters \"gamma='scale', C=100\"\n","clf = SVC(gamma = 'scale', C = 100)\n","clf.fit(X_train, Y_train)\n","\n","# evaluate the Support Vector Machine (SVM) model on test data\n","Y_pred = clf.predict(X_test)\n","accuracy = metrics.accuracy_score(Y_test, Y_pred)\n","precision_wet = metrics.precision_score(Y_test, Y_pred)\n","recall_wet = metrics.recall_score(Y_test, Y_pred)\n","F_wet = metrics.f1_score(Y_test, Y_pred)\n","\n","print(accuracy)\n","print(precision_wet)\n","print(recall_wet)\n","print(F_wet)"]},{"cell_type":"markdown","metadata":{"id":"UVQYN_s_ftSN"},"source":["<font color=red>ANSWER THE QUESTION BELOW.</font> How do you compare the results from different models above with decision tree?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1676864321252,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"5Xc1KbchftSO","outputId":"90bc34cf-fede-4011-977a-b47c3ca13ef0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","SVC model gave good results compared to all other above decision tree models as we can see the metrics.\n","\n"]}],"source":["ans = '''\n","SVC model gave good results compared to all other above decision tree models as we can see the metrics.\n","'''\n","\n","print(ans)"]},{"cell_type":"markdown","metadata":{"id":"HIrkb_jUftSO"},"source":["## Part 4: Ensemble learning\n","In this part, you will run several ensemble learning of decision trees, including bagging and random forest. For random forest, you can directly call it as a separate model from library.\n","\n","<font color=red>PLEASE COMPLETE CODES BELOW</font>. PLEASE USE THE ORIGINAL TRAINING AND TEST DATA in Part 1."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":231,"status":"ok","timestamp":1676864325682,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"cMpKk3-OftSO","outputId":"3279d7e3-72f3-459b-ba9f-21656c058d27"},"outputs":[{"name":"stdout","output_type":"stream","text":["   near infra red   red  green  blue  class\n","0              123  132    115   133      0\n","1              152  150    119   187      1\n","2              169  166    143   192      1\n","3               55   49     43    97      0\n","4              141  135    117   181      1\n","   near infra red   red  green  blue  class\n","0              137  140    129   150      0\n","1              169  162    140   193      1\n","2              124  110     89   162      1\n","3              105  104     99   153      1\n","4              105  102     88   173      1\n","0.808\n","0.7899761336515513\n","0.7609195402298851\n","0.775175644028103\n"]}],"source":["#note: class 0 for dry land, class 1 for wetland\n","col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n","features =  ['near infra red ', 'red', 'green', 'blue'] \n","\n","# load dataset\n","train_dat = pd.read_csv(\"train.csv\", header = None, names = col_names)\n","print(train_dat.head())\n","\n","test_dat = pd.read_csv(\"test.csv\", header = None, names = col_names)\n","print(test_dat.head())\n","\n","# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n","X_train = train_dat.drop('class', axis='columns')\n","Y_train = train_dat['class']\n","\n","# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n","X_test = test_dat.drop('class', axis='columns')\n","Y_test = test_dat['class']\n","\n","# please train bagging of decision tree, using BaggingClassifier with default parameters\n","clf = BaggingClassifier()\n","clf = clf.fit(X_train, Y_train)\n","\n","# please evaluate it on the test data\n","Y_pred = clf.predict(X_test)\n","accuracy = metrics.accuracy_score(Y_test, Y_pred)\n","precision_wet = metrics.precision_score(Y_test, Y_pred)\n","recall_wet = metrics.recall_score(Y_test, Y_pred)\n","F_wet = metrics.f1_score(Y_test, Y_pred)\n","\n","print(accuracy)\n","print(precision_wet)\n","print(recall_wet)\n","print(F_wet)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1676864330183,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"ZUU8DOjHftSO","outputId":"07edb936-69e5-45b8-d4a7-14b1cca6f5af"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.812\n","0.7762863534675615\n","0.7977011494252874\n","0.7868480725623582\n"]}],"source":["# please train a random forest, using RandomForestClassifier function with paramters \"n_estimators=50\"\n","clf = RandomForestClassifier(n_estimators = 50)\n","clf = clf.fit(X_train, Y_train)\n","\n","# please evaluate it on the test data\n","Y_pred = clf.predict(X_test)\n","accuracy = metrics.accuracy_score(Y_test, Y_pred)\n","precision_wet = metrics.precision_score(Y_test, Y_pred)\n","recall_wet = metrics.recall_score(Y_test, Y_pred)\n","F_wet = metrics.f1_score(Y_test, Y_pred)\n","\n","print(accuracy)\n","print(precision_wet)\n","print(recall_wet)\n","print(F_wet)"]},{"cell_type":"markdown","metadata":{"id":"XO8xMEyDftSO"},"source":["PLEASE ANSWER THE QUESTION BELOW. How do you compare different ensemble methods? Which one has the best performance?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1676864504448,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"Z1rq-GWmftSO","outputId":"07d3969d-b13d-45ab-bb42-61a5f2273f12"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Ensemble refers to methods and strategies that combine one model with several other models to improve performance, which in turn raises the machine learning model's accuracy rate.\n","\n","We distinguish the model results of the Kolmogorov-Smirnov test in order to compare various ensemble methods. The largest difference between cumulative events and cumulative non-events in the model is identified using the Kolmogorov-Smirnov test.\n","\n","With accuracy hovering around 0.81, both models perform fairly similarly, but the Bagging model slightly outperforms the Random Forest model in terms of accuracy, with a difference of only 0.002 percent.\n","\n","Furthermore, Bagging has a higher level of precision than Random Forest.\n","\n","Random Forest has a higher recall despite Bagging's superior accuracy and precision. While the Bagging model performs better overall than Random Forest.\n"]}],"source":["ans = '''\n","Ensemble refers to methods and strategies that combine one model with several other models to improve performance, which in turn raises the machine learning model's accuracy rate.\n","\n","We distinguish the model results of the Kolmogorov-Smirnov test in order to compare various ensemble methods. The largest difference between cumulative events and cumulative non-events in the model is identified using the Kolmogorov-Smirnov test.\n","\n","With accuracy hovering around 0.81, both models perform fairly similarly, but the Bagging model slightly outperforms the Random Forest model in terms of accuracy, with a difference of only 0.002 percent.\n","\n","Furthermore, Bagging has a higher level of precision than Random Forest.\n","\n","Random Forest has a higher recall despite Bagging's superior accuracy and precision. While the Bagging model performs better overall than Random Forest.'''\n","\n","print(ans)"]},{"cell_type":"markdown","metadata":{"id":"Ca4stuvTftSO"},"source":["## Part 5 (EXTRA CREDIT): Implement your own classifier (logistic regression)\n","\n","You should implement your logistic regression model: a training function and a test function. Save your source codes in 'myLR.py'. Put the python script in the same dirctory of this Jupyter Notebook. Load the scripts so that you can call your own training and prediction functions. Then evaluate the results on test data. Compare your results from the results from built-in logistic regression library in Part 4. Please use the same training data and test data as Part 1. Please make sure you print out the accuracy, confusion matrix, precision, recall, F-score. \n","\n","<b>Requirement</b>\n","\n","- Your codes should run through without bugs. Codes with bugs in Jupyter notebook running will NOT be graded. \n","    \n","- You CANNOT copy a same python codes from online for this question. It will be treated as cheating.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qkqK24Zo3GNs"},"outputs":[],"source":["# Load data\n","col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n","features =  ['near infra red ', 'red', 'green', 'blue'] \n","\n","# load dataset\n","train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n","print(train_dat.head())\n","\n","test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n","print(test_dat.head())\n","\n","# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n","X_train = \n","Y_train = \n","\n","# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n","X_test = \n","Y_test = \n","\n","# Load your implemented codes myLR.py\n","from myLR import MyLR\n","\n","# Call your own function to train a logistic regression model, as what we did in Part 1 with decision tree\n","myLR = MyLR()\n","myLR = myLR.fit(X_train, Y_train)\n","\n","# Call your own function to make prediction on test data, and evaluate those metrics, as what we did in Part 1 with deicsion tree\n","Y_pred = \n","print(Y_pred.head())\n","print(Y_pred.shape)\n","\n","# evalute the logistic regression model on test data\n","confusion_matrix = \n","accuracy = \n","precision_wet = \n","recall_wet = \n","F_wet = \n","\n","print(confusion_matrix)\n","print(accuracy)\n","print(precision_wet)\n","print(recall_wet)\n","print(F_wet)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17418,"status":"ok","timestamp":1676863677502,"user":{"displayName":"Chebolu Ratna Harika","userId":"08850792314618797682"},"user_tz":300},"id":"sVrM9jUO3LEF","outputId":"162833cf-aefc-4567-eed1-c9bb79ba5ef7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
